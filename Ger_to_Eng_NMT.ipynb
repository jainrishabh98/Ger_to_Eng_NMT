{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "import numpy as np\n",
    "from numpy import array, argmax\n",
    "from pickle import dump,load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2317\n",
      "English Max Length: 5\n",
      "German Vocabulary Size: 3686\n",
      "German Max Length: 10\n",
      "[567  20 257   0   0   0   0   0   0   0]\n",
      "[2315   30  715    0    0    0]\n",
      "[  30  715 2316    0    0    0]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[567  20 257   0   0   0   0   0   0   0]\n",
      "(9000, 10)\n",
      "(9000, 6)\n"
     ]
    }
   ],
   "source": [
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename,'rb'))\n",
    "\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    "\n",
    "def encode_sequences(tokenizer,length,lines):\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    X = pad_sequences(X,maxlen=length,padding='post')\n",
    "    print(X[0])\n",
    "    return X\n",
    "\n",
    "def encode_sequences2(tokenizer,length,lines):\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    for i in range(len(X)):\n",
    "        X[i].insert(0,2315)\n",
    "    \n",
    "    X = pad_sequences(X,maxlen=length+1,padding='post')\n",
    "    print(X[0])\n",
    "    return X\n",
    "\n",
    "def encode_sequences3(tokenizer,length,lines):\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    for i in range(len(X)):\n",
    "        X[i].append(2316)\n",
    "   \n",
    "    X = pad_sequences(X,maxlen=length+1,padding='post')\n",
    "    print(X[0])\n",
    "    return X\n",
    "\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes = vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size) #think abt it - done. compare with RNN.ipynb\n",
    "    return y\n",
    "\n",
    "# load datasets\n",
    "dataset = load_clean_sentences('\\\\english-german-both.pkl')\n",
    "train = load_clean_sentences('\\\\english-german-train.pkl')\n",
    "test = load_clean_sentences('\\\\english-german-test.pkl')\n",
    " \n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_tokenizer.word_index['strt'] = 2315\n",
    "eng_tokenizer.word_index['eos'] = 2316\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))\n",
    " \n",
    "# prepare training data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "trY = encode_sequences2(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_sequences3(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "print(trainY[0])\n",
    "print(trainX[0])\n",
    "print(trainX.shape)\n",
    "print(trY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_62 (InputLayer)           (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_63 (InputLayer)           (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_41 (Embedding)        (None, 10, 100)      368600      input_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_42 (Embedding)        (None, 6, 100)       231700      input_63[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_42 (LSTM)                  [(None, 100), (None, 80400       embedding_41[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_43 (LSTM)                  [(None, 6, 100), (No 80400       embedding_42[0][0]               \n",
      "                                                                 lstm_42[0][1]                    \n",
      "                                                                 lstm_42[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 6, 2317)      234017      lstm_43[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 995,117\n",
      "Trainable params: 995,117\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=(10,))      \n",
    "encoder_embedding = Embedding(ger_vocab_size, 100)\n",
    "encoder_inputs2 = encoder_embedding(encoder_inputs)\n",
    "encoder_lstm = LSTM(100, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs2)\n",
    "encoder_states = [state_h, state_c]\n",
    "decoder_inputs = Input(shape=(6,))\n",
    "decoder_embedding = Embedding(eng_vocab_size,100)\n",
    "decoder_inputs2 = decoder_embedding(decoder_inputs)\n",
    "decoder_lstm = LSTM(100,return_sequences = True,return_state=True)\n",
    "decoder_outputs, c, v = decoder_lstm(decoder_inputs2,initial_state=encoder_states)\n",
    "decoder_dense = Dense(eng_vocab_size,activation = 'softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "model = Model([encoder_inputs,decoder_inputs],decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "9000/9000 [==============================] - 57s 6ms/step - loss: 4.9964\n",
      "[0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rishabh\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py:872: UserWarning: Layer lstm_43 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_42/while/Exit_3:0' shape=(?, 100) dtype=float32>, <tf.Tensor 'lstm_42/while/Exit_4:0' shape=(?, 100) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "9000/9000 [==============================] - 52s 6ms/step - loss: 3.3053\n",
      "[1, 2316, 2316, 0, 0, 0]\n",
      "Epoch 3/50\n",
      "9000/9000 [==============================] - 52s 6ms/step - loss: 3.0818\n",
      "[2, 2316, 2316, 0, 0, 0]\n",
      "Epoch 4/50\n",
      "9000/9000 [==============================] - 52s 6ms/step - loss: 2.9180\n",
      "[2, 2316, 2316, 0, 0, 0]\n",
      "Epoch 5/50\n",
      "9000/9000 [==============================] - 52s 6ms/step - loss: 2.8009\n",
      "[1, 4, 2316, 0, 0, 0]\n",
      "Epoch 6/50\n",
      "9000/9000 [==============================] - 52s 6ms/step - loss: 2.7136\n",
      "[2, 6, 2316, 0, 0, 0]\n",
      "Epoch 7/50\n",
      "9000/9000 [==============================] - 52s 6ms/step - loss: 2.6426\n",
      "[2, 6, 2316, 0, 0, 0]\n",
      "Epoch 8/50\n",
      "9000/9000 [==============================] - 52s 6ms/step - loss: 2.5655\n",
      "[1, 6, 2316, 0, 0, 0]\n",
      "Epoch 9/50\n",
      "9000/9000 [==============================] - 52s 6ms/step - loss: 2.4771\n",
      "[1, 6, 2316, 0, 0, 0]\n",
      "Epoch 10/50\n",
      "9000/9000 [==============================] - 57s 6ms/step - loss: 2.3973\n",
      "[10, 6, 2316, 0, 0, 0]\n",
      "Epoch 11/50\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 2.3339\n",
      "[5, 6, 2316, 0, 0, 0]\n",
      "Epoch 12/50\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 2.2751\n",
      "[5, 6, 2316, 0, 0, 0]\n",
      "Epoch 13/50\n",
      "9000/9000 [==============================] - 61s 7ms/step - loss: 2.2160\n",
      "[19, 6, 2316, 0, 0, 0]\n",
      "Epoch 14/50\n",
      "9000/9000 [==============================] - 61s 7ms/step - loss: 2.1497\n",
      "[19, 6, 2316, 0, 0, 0]\n",
      "Epoch 15/50\n",
      "9000/9000 [==============================] - 64s 7ms/step - loss: 2.0849\n",
      "[20, 6, 2316, 0, 0, 0]\n",
      "Epoch 16/50\n",
      "9000/9000 [==============================] - 64s 7ms/step - loss: 2.0208\n",
      "[20, 6, 2316, 0, 0, 0]\n",
      "Epoch 17/50\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 1.9588\n",
      "[22, 6, 2316, 0, 0, 0]\n",
      "Epoch 18/50\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 1.8992\n",
      "[20, 28, 2316, 0, 0, 0]\n",
      "Epoch 19/50\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 1.8392\n",
      "[20, 6, 2316, 0, 0, 0]\n",
      "Epoch 20/50\n",
      "9000/9000 [==============================] - 66s 7ms/step - loss: 1.7829\n",
      "[20, 40, 2316, 0, 0, 0]\n",
      "Epoch 21/50\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 1.7295\n",
      "[22, 28, 2316, 0, 0, 0]\n",
      "Epoch 22/50\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 1.6805\n",
      "[22, 40, 2316, 0, 0, 0]\n",
      "Epoch 23/50\n",
      "9000/9000 [==============================] - 64s 7ms/step - loss: 1.6305\n",
      "[22, 40, 2316, 0, 0, 0]\n",
      "Epoch 24/50\n",
      "9000/9000 [==============================] - 64s 7ms/step - loss: 1.5849\n",
      "[22, 40, 2316, 0, 0, 0]\n",
      "Epoch 25/50\n",
      "9000/9000 [==============================] - 62s 7ms/step - loss: 1.5387\n",
      "[57, 40, 2316, 0, 0, 0]\n",
      "Epoch 26/50\n",
      "9000/9000 [==============================] - 55s 6ms/step - loss: 1.4956\n",
      "[22, 40, 2316, 0, 0, 0]\n",
      "Epoch 27/50\n",
      "9000/9000 [==============================] - 55s 6ms/step - loss: 1.4508\n",
      "[57, 84, 2316, 0, 0, 0]\n",
      "Epoch 28/50\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 1.4079\n",
      "[57, 40, 2316, 0, 0, 0]\n",
      "Epoch 29/50\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 1.3652\n",
      "[30, 84, 2316, 0, 0, 0]\n",
      "Epoch 30/50\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 1.3238\n",
      "[57, 84, 2316, 0, 0, 0]\n",
      "Epoch 31/50\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 1.2845\n",
      "[57, 84, 2316, 0, 0, 0]\n",
      "Epoch 32/50\n",
      "9000/9000 [==============================] - 61s 7ms/step - loss: 1.2460\n",
      "[57, 715, 2316, 0, 0, 0]\n",
      "Epoch 33/50\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 1.2080\n",
      "[96, 715, 2316, 0, 0, 0]\n",
      "Epoch 34/50\n",
      "9000/9000 [==============================] - 62s 7ms/step - loss: 1.1725\n",
      "[96, 715, 2316, 0, 0, 0]\n",
      "Epoch 35/50\n",
      "9000/9000 [==============================] - 61s 7ms/step - loss: 1.1348\n",
      "[96, 715, 2316, 0, 0, 0]\n",
      "Epoch 36/50\n",
      "9000/9000 [==============================] - 61s 7ms/step - loss: 1.1004\n",
      "[96, 715, 2316, 0, 0, 0]\n",
      "Epoch 37/50\n",
      "9000/9000 [==============================] - 64s 7ms/step - loss: 1.0671\n",
      "[96, 715, 2316, 0, 0, 0]\n",
      "Epoch 38/50\n",
      "9000/9000 [==============================] - 61s 7ms/step - loss: 1.0338\n",
      "[96, 715, 2316, 0, 0, 0]\n",
      "Epoch 39/50\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 1.0026\n",
      "[96, 715, 2316, 0, 0, 0]\n",
      "Epoch 40/50\n",
      "9000/9000 [==============================] - 62s 7ms/step - loss: 0.9741\n",
      "[96, 715, 2316, 0, 0, 0]\n",
      "Epoch 41/50\n",
      "9000/9000 [==============================] - 62s 7ms/step - loss: 0.9435\n",
      "[96, 715, 2316, 0, 0, 0]\n",
      "Epoch 42/50\n",
      "9000/9000 [==============================] - 61s 7ms/step - loss: 0.9129\n",
      "[96, 715, 2316, 0, 0, 0]\n",
      "Epoch 43/50\n",
      "9000/9000 [==============================] - 62s 7ms/step - loss: 0.8840\n",
      "[96, 715, 2316, 0, 0, 0]\n",
      "Epoch 44/50\n",
      "9000/9000 [==============================] - 61s 7ms/step - loss: 0.8580\n",
      "[96, 715, 2316, 0, 0, 0]\n",
      "Epoch 45/50\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 0.8306\n",
      "[96, 715, 2316, 0, 0, 0]\n",
      "Epoch 46/50\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 0.8052\n",
      "[96, 715, 2316, 0, 0, 0]\n",
      "Epoch 47/50\n",
      "9000/9000 [==============================] - 62s 7ms/step - loss: 0.7785\n",
      "[96, 715, 2316, 0, 0, 0]\n",
      "Epoch 48/50\n",
      "9000/9000 [==============================] - 62s 7ms/step - loss: 0.7557\n",
      "[96, 715, 2316, 0, 0, 0]\n",
      "Epoch 49/50\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 0.7310\n",
      "[96, 715, 2316, 0, 0, 0]\n",
      "Epoch 50/50\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.7069\n",
      "[96, 715, 2316, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x4663d438>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class prttt(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        temp = trainX[0].reshape((1,trainX[0].shape[0]))\n",
    "        temp2 = trY[0].reshape((1,trY[0].shape[0]))\n",
    "        prediction = model.predict([temp,temp2])[0]\n",
    "        #print(prediction)\n",
    "        integer = [argmax (vector) for vector in prediction]\n",
    "        print(integer)\n",
    "prt = prttt()\n",
    "\n",
    "filepath = \"D:\\\\model2.h5\"\n",
    "checker = keras.callbacks.ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "model.fit([trainX, trY], trainY,\n",
    "          batch_size=128,\n",
    "          epochs=50,\n",
    "         callbacks = [prt,checker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_68 (InputLayer)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_41 (Embedding)     (None, 10, 100)           368600    \n",
      "_________________________________________________________________\n",
      "lstm_42 (LSTM)               [(None, 100), (None, 100) 80400     \n",
      "=================================================================\n",
      "Total params: 449,000\n",
      "Trainable params: 449,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_71 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_42 (Embedding)        multiple             231700      input_71[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_69 (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_70 (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_43 (LSTM)                  multiple             80400       embedding_42[2][0]               \n",
      "                                                                 input_69[0][0]                   \n",
      "                                                                 input_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                multiple             234017      lstm_43[2][0]                    \n",
      "==================================================================================================\n",
      "Total params: 546,117\n",
      "Trainable params: 546,117\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "################## MODEL FOR TESTING #########################\n",
    "\n",
    "encoder_inputs = Input(shape=(10,))\n",
    "encoder_inputs2 = encoder_embedding(encoder_inputs)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs2)\n",
    "encoder_states = [state_h, state_c]\n",
    "encoder_model = Model(encoder_inputs,encoder_states)\n",
    "encoder_model.summary()\n",
    "decoder_state_input_h = Input(shape=(100,))\n",
    "decoder_state_input_c = Input(shape=(100,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_inputs = Input(shape=(1,))\n",
    "decoder_inputs2 = decoder_embedding(decoder_inputs)\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs2, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "decoder_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = [2315] #start token\n",
    "    stop_condition = False\n",
    "    decoded_sentence = list()\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "        \n",
    "        #print(output_tokens)\n",
    "        integers = [argmax(vector) for vector in output_tokens[0]]\n",
    "        #print(integers)\n",
    "        target_seq = integers\n",
    "        sampled_char = ''\n",
    "        for j in integers:\n",
    "            word = word_for_id(j, eng_tokenizer)\n",
    "            if word is 'eos' or word is None:\n",
    "                sampled_char = word\n",
    "                break\n",
    "            decoded_sentence.append(word)\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == 'eos' or sampled_char == None or len(decoded_sentence) > 5:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return ' '.join(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction on train\n",
      "src=[fass dich kurz], target=[be brief], predicted=[wait up us]\n",
      "src=[geh jetzt], target=[leave now], predicted=[lets go]\n",
      "src=[mir gehts gut], target=[im fine], predicted=[im doing ok]\n",
      "src=[ich erinnere mich an sie], target=[i remember them], predicted=[i dont see]\n",
      "src=[tom amusiert sich], target=[toms amused], predicted=[toms engaged]\n",
      "src=[du schaffst das], target=[youll make it], predicted=[youll do it]\n",
      "src=[ich liebe den herbst], target=[i love autumn], predicted=[i love math]\n",
      "src=[tom ist ein chaot], target=[tom is a slob], predicted=[tom is a lot]\n",
      "src=[ich mag mathe], target=[i like math], predicted=[i like to dance]\n",
      "src=[magst du mich], target=[do you like me], predicted=[do you love me]\n"
     ]
    }
   ],
   "source": [
    "print('prediction on train')\n",
    "for i in range(0,10):\n",
    "    temp = trainX[i].reshape((1,trainX[i].shape[0]))\n",
    "    translation = decode_sequence(temp)\n",
    "    raw_target, raw_src = train[i]\n",
    "    print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1 227   4  95   0   0   0   0   0   0]\n",
      "prediction on test\n",
      "src=[ich kenne sie alle], target=[i know them all], predicted=[i know her car]\n",
      "src=[der mann erotete], target=[the man blushed], predicted=[the man is hot]\n",
      "src=[ist dir nicht kalt], target=[arent you cold], predicted=[arent you hot]\n",
      "src=[hor auf zu schreien], target=[stop yelling], predicted=[stop please]\n",
      "src=[hier ist tom], target=[here comes tom], predicted=[is it here]\n",
      "src=[dies ist mein schreibtisch], target=[this is my desk], predicted=[this is my plan]\n",
      "src=[ist das eures], target=[is it yours], predicted=[is it yours]\n",
      "src=[er ist mein vater], target=[he is my father], predicted=[hes a grouch]\n",
      "src=[mir tut alles weh], target=[i ache all over], predicted=[im afraid]\n",
      "src=[tom veranderte sich], target=[tom changed], predicted=[tom will work]\n"
     ]
    }
   ],
   "source": [
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "print('prediction on test')\n",
    "for i in range(0,10):\n",
    "    temp = testX[i].reshape((1,testX[i].shape[0]))\n",
    "    translation = decode_sequence(temp)\n",
    "    raw_target, raw_src = test[i]\n",
    "    print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
